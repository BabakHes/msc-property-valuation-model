{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6679,
     "status": "ok",
     "timestamp": 1648684754764,
     "user": {
      "displayName": "Melro Leandro",
      "userId": "06062551217768606324"
     },
     "user_tz": -60
    },
    "id": "8lTcjjPhpsoo",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "83654905-9359-46fc-809d-ec7f6435f34a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import median_absolute_error as MAPE\n",
    "from sklearn.metrics import r2_score  as R2\n",
    "from xgboost import plot_importance\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Parameter tunning\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "%matplotlib inline\n",
    "#*Version*: 2\n",
    "\n",
    "#Data description:\n",
    "#- 1127 postcodes\n",
    "#- Google POI + EPC statistics + Property Data\n",
    "#- New buiding ranges\n",
    "\n",
    "#Goal: Rents\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "Data = pd.read_csv('/content/drive/My Drive/datasetV6.csv') \n",
    "\n",
    "list(Data.columns)\n",
    "\n",
    "# Crime\n",
    "Data['crime_total']=Data['PublicOrder']+Data['Burglary']+Data['Robbery']+Data['PossessionOfWeapons']+Data['BicycleTheft']+Data['AntiSocialBehaviour']+Data['Violence']+Data['Theft']+Data['Shoplifting']+Data['OtherCrime']+Data['Drugs']+Data['OtherTheft']+Data['VehicleCrime']+Data['CriminalDamage']\n",
    "Data['PublicOrder']=Data['PublicOrder']/Data['crime_total']\n",
    "Data['Burglary'] = Data['Burglary']/Data['crime_total']\n",
    "Data['Robbery'] = Data['Robbery']/Data['crime_total']\n",
    "Data['PossessionOfWeapons'] = Data['PossessionOfWeapons']/Data['crime_total']\n",
    "Data['BicycleTheft'] = Data['BicycleTheft']/Data['crime_total']\n",
    "Data['AntiSocialBehaviour'] = Data['AntiSocialBehaviour']/Data['crime_total']\n",
    "Data['Violence'] = Data['Violence']/Data['crime_total']\n",
    "Data['Theft'] = Data['Theft']/Data['crime_total']\n",
    "Data['Shoplifting'] = Data['Shoplifting']/Data['crime_total']\n",
    "Data['OtherCrime'] = Data['OtherCrime']/Data['crime_total']\n",
    "Data['Drugs'] = Data['Drugs']/Data['crime_total']\n",
    "Data['OtherTheft'] = Data['OtherTheft']/Data['crime_total']\n",
    "Data['VehicleCrime'] = Data['VehicleCrime']/Data['crime_total']\n",
    "Data['CriminalDamage'] = Data['CriminalDamage']/Data['crime_total']\n",
    "\n",
    "Data['crime_Type_A'] = Data['Robbery']+Data['Drugs']+Data['PossessionOfWeapons']+Data['Burglary']+Data['OtherCrime']\n",
    "Data['crime_Type_B'] = Data['OtherTheft'] + Data['Shoplifting'] + Data['Theft'] + Data['BicycleTheft']\n",
    "Data['crime_Type_C'] = Data['CriminalDamage']+ Data['Violence'] + Data['AntiSocialBehaviour']\n",
    "Data['crime_Type_D'] = Data['VehicleCrime']\n",
    "dropList=['Burglary',\n",
    " 'Robbery',\n",
    " 'PossessionOfWeapons',\n",
    " 'BicycleTheft',\n",
    " 'AntiSocialBehaviour',\n",
    " 'Violence',\n",
    " 'Theft',\n",
    " 'Shoplifting',\n",
    " 'OtherCrime',\n",
    " 'Drugs',\n",
    " 'OtherTheft',\n",
    " 'VehicleCrime',\n",
    " 'CriminalDamage',]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "\n",
    "Data['TotalModPop'] = Data['age20_24']+Data['age25_29']+Data['age30_34']+Data['age35_39']+Data['age40_44']+Data['age45_49']+Data['age45_49']+Data['age50_54']+Data['age60_64']\n",
    "Data['age20_29'] = Data['age20_24']+Data['age25_29']\n",
    "Data['age30_64'] = Data['age30_34']+Data['age35_39']+Data['age40_44']+Data['age45_49']+Data['age50_54']+Data['age55_59']+Data['age60_64']\n",
    "Data['age65_'] = Data['age70_74']+Data['age75_79']+Data['age80_84']+Data['age85_89']\n",
    "dropList=['age20_24',\n",
    " 'age25_29',\n",
    " 'age30_34',\n",
    " 'age35_39',\n",
    " 'age40_44',\n",
    " 'age45_49',\n",
    " 'age50_54',\n",
    " 'age55_59',\n",
    " 'age60_64',\n",
    " #'age65-69',\n",
    " 'age70_74',\n",
    " 'age75_79',\n",
    " 'age80_84',\n",
    " 'age85_89']\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "\n",
    "# Commute method\n",
    "Data['commute_method_public'] = Data['commute_method_train'] + Data['commute_method_bus'] + Data['commute_method_underground_light_rail']\n",
    "Data['commute_method_private'] = Data['commute_method_motorcycle'] + Data['commute_method_taxi'] + Data['commute_method_car_driver'] + Data['commute_method_car_passenger']\n",
    "Data['commute_method_open'] = Data['commute_method_foot']+Data['commute_method_bicycle']\n",
    "dropList=['commute_method_foot',\n",
    " 'commute_method_bicycle',\n",
    " 'commute_method_other',\n",
    " 'commute_method_motorcycle',\n",
    " 'commute_method_taxi',\n",
    " 'commute_method_train',\n",
    " 'commute_method_bus',\n",
    " 'commute_method_underground_light_rail',\n",
    " 'commute_method_car_driver',\n",
    " 'commute_method_at_home',\n",
    " 'commute_method_car_passenger',\n",
    "]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "#Data['sold_70pc_disp']=-(Data['sold_70pc_rangeMIN']-Data['sold_70pc_rangeMAX'])/Data['sold_average']\n",
    "#Data['sold_80pc_disp']=-(Data['sold_80pc_rangeMIN']-Data['sold_80pc_rangeMAX'])/Data['sold_average']\n",
    "#Data['sold_90pc_disp']=-(Data['sold_90pc_rangeMIN']-Data['sold_90pc_rangeMAX'])/Data['sold_average']\n",
    "#Data['sold_100pc_disp']=-(Data['sold_100pc_rangeMIN']-Data['sold_100pc_rangeMAX'])/Data['sold_average']\n",
    "#Data['sold_sqf_70pc_disp'] =-(Data['sold_sqf_70pc_rangeMIN']-Data['sold_sqf_70pc_rangeMAX'])/Data['sold_sqf_average']\n",
    "#Data['sold_sqf_80pc_disp'] =-(Data['sold_sqf_80pc_rangeMIN']-Data['sold_sqf_80pc_rangeMAX'])/Data['sold_sqf_average']\n",
    "#Data['sold_sqf_90pc_disp'] =-(Data['sold_sqf_90pc_rangeMIN']-Data['sold_sqf_90pc_rangeMAX'])/Data['sold_sqf_average']\n",
    "#Data['sold_sqf_100pc_disp'] =-(Data['sold_sqf_100pc_rangeMIN']-Data['sold_sqf_100pc_rangeMAX'])/Data['sold_sqf_average']\n",
    "#Data['rents_70pc_disp'] = -(Data['rents_70pc_rangeMIN']-Data['rents_70pc_rangeMAX'])/Data['rents_average']\n",
    "#Data['rents_80pc_disp'] =-(Data['rents_80pc_rangeMIN']-Data['rents_80pc_rangeMAX'])/Data['rents_average']\n",
    "#Data['rents_90pc_disp'] =-(Data['rents_90pc_rangeMIN']-Data['rents_90pc_rangeMAX'])/Data['rents_average']\n",
    "#Data['rents_100pc_disp'] =-(Data['rents_100pc_rangeMIN']-Data['rents_100pc_rangeMAX'])/Data['rents_average']\n",
    "# Attributes responsible to overfiting\n",
    "list(Data.columns)\n",
    "\n",
    "dropList=[ 'dyn_sold_points_analysed',\n",
    " 'dyn_sold_70pc_rangeMIN',\n",
    " 'dyn_sold_70pc_rangeMAX',\n",
    " 'dyn_sold_80pc_rangeMIN',\n",
    " 'dyn_sold_80pc_rangeMAX',\n",
    " 'dyn_sold_90pc_rangeMIN',\n",
    " 'dyn_sold_90pc_rangeMAX',\n",
    " 'dyn_sold_sqf_points_analysed',\n",
    " 'dyn_sold_sqf_70pc_rangeMIN',\n",
    " 'dyn_sold_sqf_70pc_rangeMAX',\n",
    " 'dyn_sold_sqf_80pc_rangeMIN',\n",
    " 'dyn_sold_sqf_80pc_rangeMAX',\n",
    " 'dyn_sold_sqf_90pc_rangeMIN',\n",
    " 'dyn_sold_sqf_90pc_rangeMAX',\n",
    " 'dyn_rents_points_analysed',\n",
    " 'dyn_rents_70pc_rangeMIN',\n",
    " 'dyn_rents_70pc_rangeMAX',\n",
    " 'dyn_rents_80pc_rangeMIN',\n",
    " 'dyn_rents_80pc_rangeMAX',\n",
    " 'dyn_rents_90pc_rangeMIN',\n",
    " 'dyn_rents_90pc_rangeMAX',]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "\n",
    "# Density\n",
    "Data['HousePerPopulation']=Data['Households']/Data['Population']\n",
    "\n",
    "# Construction\n",
    "#'construction_before_1900',\n",
    "#'construction_1900_1929',\n",
    "#'construction_1930_1949',\n",
    "Data['construction_1950_1975']=(Data['construction_1950_1966']+Data['construction_1967_1975']+ Data['construction_1976_1982'])\n",
    "#'construction_1976_1982',\n",
    "Data['construction_1983_1995']=(Data['construction_1983_1990']+Data['construction_1991_1995'])\n",
    "Data['construction_1996_2006']=(Data['construction_1996_2002']+Data['construction_2003_2006'])\n",
    "#'construction_2007_2011'\n",
    "#'construction_2012_onwards'\n",
    "dropList=[\n",
    " #'construction_before_1900',\n",
    " #'construction_1900_1929',\n",
    " #'construction_1930_1949',\n",
    " 'construction_1950_1966',\n",
    " 'construction_1967_1975',\n",
    " #'construction_1976_1982',\n",
    " 'construction_1983_1990',\n",
    " 'construction_1991_1995',\n",
    " 'construction_1996_2002',\n",
    " 'construction_2003_2006',\n",
    " #'construction_2007_2011',\n",
    " #'construction_2012_onwards'\n",
    " ]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "list(Data.columns)\n",
    "# Removing inrelevant attributes\n",
    "dropList=['Unnamed: 0','postcode','count',\n",
    "         'postcode_Latitude','postcode_Longitude','MSOA_Code','LSOA_Code','area'\n",
    "          ]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "\n",
    "dropList=['dyn_sold_average',\n",
    " 'dyn_sold_100pc_rangeMIN',\n",
    " 'dyn_sold_100pc_rangeMAX',\n",
    " 'dyn_sold_sqf_100pc_rangeMIN',\n",
    " 'dyn_sold_sqf_100pc_rangeMAX',\n",
    " 'dyn_sold_sqf_average',\n",
    " 'dyn_rents_100pc_rangeMIN',\n",
    " 'dyn_rents_100pc_rangeMAX',\n",
    "         ]\n",
    "Data.drop(dropList, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Rental price statistics\n",
    "rental_mean = Data['dyn_rents_average'].mean()\n",
    "\n",
    "# Categorical attribute binarization\n",
    "catData = ['dyn_ptal', 'dyn_flood_risk','crime_rating']\n",
    "for att in catData:\n",
    "    unique_classes = Data[att].unique()\n",
    "    one_hot = pd.get_dummies(unique_classes, prefix=att,dummy_na=True, columns = [att])\n",
    "    one_hot[att] = unique_classes\n",
    "    Data = Data.merge(one_hot, on = [att], how='left')\n",
    "    Data = Data.drop(columns = [att])\n",
    "\n",
    "# Define rental price as the model goal \n",
    "Data['price'] = Data['dyn_rents_average']\n",
    "Data.drop(['dyn_rents_average'], axis = 1, inplace = True)\n",
    "\n",
    "# Split data: X,y\n",
    "X, y = Data.iloc[:, :-1], Data.iloc[:, -1]\n",
    "\n",
    "# Model tunning\n",
    "if False:\n",
    "  def objective(trial, X, y, kf):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 206, 512)\n",
    "    learning_rate =trial.suggest_uniform('learning_rate', 0, 0.1) \n",
    "    #max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_split_loss = trial.suggest_uniform('min_split_loss', 0, 2.5)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    reg_lambda = trial.suggest_uniform('reg_lambda', 0, 10)\n",
    "    reg_alpha = trial.suggest_uniform('reg_alpha', 0, 10)\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.5, 1)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.5, 1)\n",
    "    eta = trial.suggest_uniform('eta', 0, 1)\n",
    "    \n",
    "    model = xg.XGBRegressor(n_estimators=n_estimators,\n",
    "                          #max_depth=max_depth,\n",
    "                          learning_rate=learning_rate,\n",
    "                          min_split_loss=min_split_loss,\n",
    "                          min_child_weight=min_child_weight,\n",
    "                          reg_lambda=reg_lambda,\n",
    "                          reg_alpha=reg_alpha,\n",
    "                          colsample_bytree=colsample_bytree,\n",
    "                          subsample=subsample,\n",
    "                          eta=eta,\n",
    "                          objective='reg:squarederror', \n",
    "                          booster='gbtree',\n",
    "                          seed=42,\n",
    "                          n_jobs=3)\n",
    "    \n",
    "    \n",
    "    test_score = []\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        test_score.append(MAE(y_test, y_pred)/rental_mean*100)\n",
    "        \n",
    "        trial.report(np.mean(test_score), step)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "            \n",
    "        step+=1\n",
    "\n",
    "    return np.mean(test_score)\n",
    "\n",
    "\n",
    "\n",
    "  kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "  study = optuna.create_study(direction='minimize',\n",
    "                              pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "\n",
    "  study.optimize(lambda trial: objective(trial, X, y, kf), timeout=30*60, n_jobs=1)\n",
    "\n",
    "  study.best_params \n",
    "\n",
    "# Splitting Train/Test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                      test_size = 0.2)\n",
    "# Model\n",
    "model = XGBRegressor(objective='reg:squarederror', \n",
    "                     n_estimators=  492,\n",
    "                     learning_rate= 0.055149054063114315,\n",
    "                     min_split_loss= 1.0150944845565792,\n",
    "                     min_child_weight= 1,\n",
    "                     reg_lambda= 4.607602881665805,\n",
    "                     reg_alpha= 9.61243898655445,\n",
    "                     colsample_bytree= 0.6947298833072163,\n",
    "                     subsample=   0.7305682818662972,\n",
    "                     eta= 0.8335613259248802)\n",
    "\n",
    "xgb_r = model.fit(train_X, train_y)\n",
    "\n",
    "pred_train = model.predict(train_X)\n",
    "pred_test = model.predict(test_X)\n",
    "\n",
    "# MAE Computation\n",
    "mae =MAE(train_y, pred_train)\n",
    "print(\"Train MAE : % f --- %f\" %(mae,mae/rental_mean*100),\"%\")\n",
    "# MAE Computation\n",
    "mae =MAE(test_y, pred_test)\n",
    "print(\"Test MAE : % f --- %f\" %(mae,mae/rental_mean*100),\"%\")\n",
    "\n",
    "# MAE Computation\n",
    "mape =MAPE(train_y, pred_train)\n",
    "print(\"Train MAPE : % f --- %f\" %(mape,mape/rental_mean*100),\"%\")\n",
    "# MAE Computation\n",
    "mape =MAPE(test_y, pred_test)\n",
    "print(\"Test MAPE : % f --- %f\" %(mape,mape/rental_mean*100),\"%\")\n",
    "\n",
    "# MAE Computation\n",
    "r2 =R2(train_y, pred_train)\n",
    "print(\"Train R2 : % f\" %(r2))\n",
    "# MAE Computation\n",
    "r2 =R2(test_y, pred_test)\n",
    "print(\"Test R2 : % f\" %(r2))\n",
    "# Train MAE :  2.798894 --- 0.682709 %\n",
    "# Test MAE :  9.621230 --- 2.346819 %\n",
    "# Train MAPE :  2.364685 --- 0.576796 %\n",
    "# Test MAPE :  7.469833 --- 1.822049 %\n",
    "# Train R2 :  0.997107\n",
    "# Test R2 :  0.965415\n",
    "sns.set_style('whitegrid')\n",
    "sns.displot( abs(test_y-pred_test)/test_y*100, kde = False, color ='red', bins = 30)\n",
    "plt.legend(title='Relative Error distribution in %')\n",
    "\n",
    "# Cross validation\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "# MAE Computation\n",
    "print(\"CV MAE : £\" ,scores.mean(), scores.mean()/rental_mean*100,\" % --- std £\",scores.std())\n",
    "#CV MAE : £ 9.29957054490235 2.268359757733832  % --- std £ 0.#7142599550453957\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "\n",
    "# R2 Computation\n",
    "print(\"CV R2 :\" ,scores.mean(), \" std:\",scores.std())\n",
    "# CV R2 : 0.9624636849509701  std: 0.005759984027051642"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7HmNZFva6yiD",
    "RHi6VVPa6yij",
    "cKagakwK6yis",
    "bF3wzxC_6yix",
    "90TllzRP6yjG",
    "-DOltKLx6yjH",
    "fu004M516yjH",
    "anoB3O7k6yjR",
    "ko-ruQXN6yjR",
    "yu6Ym2SH6yjS",
    "tJfc9TI96yjT",
    "VpuYZXfn6yjd"
   ],
   "name": "NewRentalModelPlacesV2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
